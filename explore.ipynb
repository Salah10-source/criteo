{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3708dc7c",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18c2313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langdetect import detect, DetectorFactory\n",
    "import torch\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "# Open file in read mode\n",
    "\n",
    "taxonomy_path = os.path.join(\"data\", \"taxonomy.txt\")\n",
    "count_of_products_per_level1_path = os.path.join(\"data\", \"count_of_products_per_level1.csv\")\n",
    "data_path = os.path.join(\"data\", \"ensae_export_without_l1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c0f6f",
   "metadata": {},
   "source": [
    "## Read categories files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242190c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 categories: ['Goods', 'toys & games', 'mature', 'cameras & optics', 'Real Estate', 'health & beauty', 'religious & ceremonial', 'Communication', 'home & garden', 'food, beverages & tobacco', 'sporting goods', 'Airlines', 'Ground/Cruises/Packages', 'animals & pet supplies', 'Gaming/Gambling', 'Hotels/Resorts', 'business & industrial', 'Finance Services', 'luggage & bags', 'Employment', 'office supplies', 'Travel', 'Car Rental', 'vehicles & parts', 'apparel & accessories', 'electronics', 'Services', 'furniture', 'software', 'baby & toddler', 'arts & entertainment', 'hardware', 'media', 'Dating']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_1_category</th>\n",
       "      <th>concatenated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goods</td>\n",
       "      <td>Goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toys &amp; games</td>\n",
       "      <td>action &amp; toy figures activity toys air &amp; water...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mature</td>\n",
       "      <td>ammunition ammunition cases &amp; holders ammuniti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cameras &amp; optics</td>\n",
       "      <td>binocular &amp; monocular accessories binoculars b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Real Estate</td>\n",
       "      <td>Real Estate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_1_category                                  concatenated_text\n",
       "0             Goods                                              Goods\n",
       "1      toys & games  action & toy figures activity toys air & water...\n",
       "2            mature  ammunition ammunition cases & holders ammuniti...\n",
       "3  cameras & optics  binocular & monocular accessories binoculars b...\n",
       "4       Real Estate                                        Real Estate"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------- Lire la taxonomy depuis un fichier txt ----------\n",
    "# Assumons que le fichier s'appelle \"taxonomy.txt\"\n",
    "# Format attendu : id_path <tab> category_path\n",
    "df_taxonomy = pd.read_csv(taxonomy_path, sep='\\t', header=None, names=['id_path', 'category_path'])\n",
    "\n",
    "# Nettoyage\n",
    "df_taxonomy['category_path'] = df_taxonomy['category_path'].str.strip()\n",
    "df_taxonomy['id_path'] = df_taxonomy['id_path'].str.strip()\n",
    "\n",
    "# Construire le graphe dirigé de la taxonomie\n",
    "G = nx.DiGraph()\n",
    "root = \"ROOT\"  # racine commune\n",
    "G.add_node(root)\n",
    "\n",
    "for path in df_taxonomy['category_path']:\n",
    "    parts = [p.strip() for p in path.split(\">\")]\n",
    "    if parts:  # relier le level_1 à la racine\n",
    "        G.add_edge(root, parts[0])\n",
    "    for i in range(len(parts)-1):\n",
    "        parent = parts[i]\n",
    "        child = parts[i+1]\n",
    "        G.add_edge(parent, child)\n",
    "\n",
    "# Identifier les level_1\n",
    "level_1_nodes = [p.split(\">\")[0].strip() for p in df_taxonomy['category_path']]\n",
    "level_1_nodes = list(set(level_1_nodes))\n",
    "\n",
    "print(\"Level 1 categories:\", level_1_nodes)\n",
    "\n",
    "# Créer un DataFrame par level_1 avec la concaténation de tous ses descendants\n",
    "level1_texts = {}\n",
    "for lvl1 in level_1_nodes:\n",
    "    descendants = nx.descendants(G, lvl1)\n",
    "    all_nodes = list(descendants) + [lvl1]\n",
    "    # enlever doublons et concaténer\n",
    "    text = \" \".join(sorted(set(all_nodes)))\n",
    "    level1_texts[lvl1] = text\n",
    "\n",
    "df_level1 = pd.DataFrame.from_dict(level1_texts, orient='index', columns=['concatenated_text'])\n",
    "df_level1.reset_index(inplace=True)\n",
    "df_level1 = df_level1.rename(columns={'index': 'level_1_category'})\n",
    "\n",
    "display(df_level1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8af4e86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 13 level 1 categories not in count_of_products_per_level1.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract level 1 category counts\n",
    "\n",
    "df_categories_count = pd.read_csv(count_of_products_per_level1_path)\n",
    "df_categories_count\n",
    "\n",
    "# Keep only level 1 categories present in both dataframes\n",
    "common_level1 = set(df_level1['level_1_category']).intersection(set(df_categories_count['level_1_name']))\n",
    "df_level1_clean = df_level1[df_level1['level_1_category'].isin(common_level1)]\n",
    "\n",
    "print(f\"Drop {len(df_level1['level_1_category'].unique()) - len(common_level1)} level 1 categories not in count_of_products_per_level1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6c2c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products :  128253\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of products : \", df_categories_count[\"count\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2d4f7",
   "metadata": {},
   "source": [
    "## Read catalog files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70153774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalog = pd.read_parquet(data_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574ef9c",
   "metadata": {},
   "source": [
    "## Preprocessing df_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb2d7e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashed_external_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>brand_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2772291400701920348</td>\n",
       "      <td>the hoodoo tarot a divination deck and guidebo...</td>\n",
       "      <td>35.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4184851053829790189</td>\n",
       "      <td>disney villains tarot deck and guidebook movie...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8778697834751578524</td>\n",
       "      <td>easy tarot created especially for beginners th...</td>\n",
       "      <td>19.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3541475158234224984</td>\n",
       "      <td>the proudest blue a story of hijab and family ...</td>\n",
       "      <td>17.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2529310467283008815</td>\n",
       "      <td>the crystal magic tarot understand and control...</td>\n",
       "      <td>24.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashed_external_id                                               text  \\\n",
       "0 -2772291400701920348  the hoodoo tarot a divination deck and guidebo...   \n",
       "1 -4184851053829790189  disney villains tarot deck and guidebook movie...   \n",
       "2 -8778697834751578524  easy tarot created especially for beginners th...   \n",
       "3 -3541475158234224984  the proudest blue a story of hijab and family ...   \n",
       "4 -2529310467283008815  the crystal magic tarot understand and control...   \n",
       "\n",
       "   sale_price  brand_encoded  \n",
       "0       35.00              0  \n",
       "1       24.99              0  \n",
       "2       19.95              0  \n",
       "3       17.99              0  \n",
       "4       24.95              0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_for_nlp(df, text_cols=['title', 'description'], brand_col='brand', id_col='hashed_external_id' , price_col = 'sale_price'):\n",
    "    \"\"\"\n",
    "    Preprocess products DataFrame for NLP tasks:\n",
    "    - Concatenate text columns into a single 'text' column\n",
    "    - Encode brand as integer\n",
    "    - Clean text: lowercasing, remove punctuation, multiple spaces\n",
    "    - Keep hashed_external_id for final output\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    text_cols : list of str\n",
    "        Columns to concatenate for text\n",
    "    brand_col : str\n",
    "        Column to encode as integer\n",
    "    id_col : str\n",
    "        Column to keep for external mapping\n",
    "    price_col : str\n",
    "        Column for price information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_preprocessed : pd.DataFrame\n",
    "        DataFrame with columns: 'hashed_external_id', 'text', brand_col (encoded)\n",
    "    label_enc : LabelEncoder\n",
    "        Fitted LabelEncoder for the brand column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Fill missing values for text columns\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].fillna('')\n",
    "\n",
    "    # Concatenate text columns\n",
    "    df['text'] = df[text_cols].agg(' '.join, axis=1)\n",
    "\n",
    "    # Clean text\n",
    "    def clean_text(s):\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'\\s+', ' ', s)      # multiple spaces -> single space\n",
    "        s = re.sub(r'[^\\w\\s]', '', s)   # remove punctuation\n",
    "        return s.strip()\n",
    "\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "    # Encode brand as integer\n",
    "    if brand_col in df.columns:\n",
    "        df[brand_col] = df[brand_col].fillna('Unknown')\n",
    "        label_enc = LabelEncoder()\n",
    "        df[brand_col + '_encoded'] = label_enc.fit_transform(df[brand_col])\n",
    "    else:\n",
    "        label_enc = None\n",
    "\n",
    "    # Fill na for price column\n",
    "    if price_col in df.columns:\n",
    "        df[price_col] = df[price_col].astype(float)\n",
    "        df[price_col] = df[price_col].fillna(df[price_col].median())\n",
    "\n",
    "    # Keep hashed_external_id\n",
    "    columns_to_keep = [id_col, 'text' , price_col]\n",
    "    if label_enc:\n",
    "        columns_to_keep.append(brand_col + '_encoded')\n",
    "\n",
    "    return df[columns_to_keep], label_enc\n",
    "\n",
    "# --------- Usage ---------\n",
    "df_nlp, brand_encoder = preprocess_for_nlp(\n",
    "    df_catalog,\n",
    "    text_cols=['title','description','brand'],\n",
    "    brand_col='brand',\n",
    "    id_col='hashed_external_id',\n",
    "    price_col = 'sale_price'\n",
    ")\n",
    "\n",
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47812d0",
   "metadata": {},
   "source": [
    "### check language distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7080ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour rendre les résultats reproductibles\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect the language of a given text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lang_code : str\n",
    "        ISO 639-1 language code (e.g., 'en' for English)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "language_df_level1_clean = df_level1_clean[\"concatenated_text\"].apply(detect_language)\n",
    "language_df_nlp = df_nlp[\"text\"].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16998097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "en    128107\n",
       "es        37\n",
       "fr        30\n",
       "it        20\n",
       "ca        16\n",
       "no        10\n",
       "af         8\n",
       "nl         7\n",
       "da         5\n",
       "ro         4\n",
       "tl         2\n",
       "sv         2\n",
       "lt         2\n",
       "cy         2\n",
       "et         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_df_level1_clean.value_counts()\n",
    "language_df_nlp.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b1c2b5",
   "metadata": {},
   "source": [
    "the vast majority of the text seems to be in English, with some other languages mixed in.\n",
    "\n",
    "Decide to drop non-English entries for simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60bfc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_df_level1_clean.value_counts().to_csv(\"data/language_distribution_level1_clean_count.csv\", index=True)\n",
    "language_df_nlp.value_counts().to_csv(\"data/language_distribution_nlp_count.csv\", index=True)\n",
    "\n",
    "language_df_level1_clean.to_csv(\"data/language_distribution_level1_clean.csv\",index=True,columns=[\"idx\",\"language\"])\n",
    "language_df_nlp.to_csv(\"data/language_df_nlp.csv\" , index=True , columns=[\"idx\",\"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd33ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop non-english entries\n",
    "\n",
    "language_df_nlp = pd.read_csv(\"data/language_df_nlp.csv\")\n",
    "df_nlp = df_nlp[language_df_nlp[\"language\"] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6ec4c",
   "metadata": {},
   "source": [
    "# Generate embeddings with sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e3980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy embeddings shape: (21, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- 1. Choisir le device ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------- 2. Charger le modèle ----------\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# ---------- 3. Préparer les textes ----------\n",
    "texts_catalog = df_nlp['text'].tolist()\n",
    "texts_taxonomy = [\n",
    "    lvl1 + \" \" + text\n",
    "    for lvl1, text in zip(df_level1_clean['level_1_category'], df_level1_clean['concatenated_text'])\n",
    "]\n",
    "\n",
    "# ---------- 4. Encode avec batch et tqdm ----------\n",
    "def encode_texts(texts, batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_emb = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_numpy=True,\n",
    "            device=device,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        embeddings.append(batch_emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "emb_catalog = encode_texts(texts_catalog, batch_size=64)\n",
    "emb_taxonomy = encode_texts(texts_taxonomy, batch_size=64)\n",
    "\n",
    "print(\"Catalogue embeddings shape:\", emb_catalog.shape)\n",
    "print(\"Taxonomy embeddings shape:\", emb_taxonomy.shape)\n",
    "\n",
    "# ---------- 5. Sauvegarde compressée ----------\n",
    "np.savez_compressed(\"data/emb_catalog.npz\", emb_catalog)\n",
    "np.savez_compressed(\"data/emb_taxonomy.npz\", emb_taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf28a7",
   "metadata": {},
   "source": [
    "## Save embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c6b434d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00704797, -0.08531265,  0.02309315, ...,  0.02168325,\n",
       "        -0.01170662,  0.0326249 ],\n",
       "       [-0.00548955, -0.0460981 ,  0.01709225, ..., -0.07944578,\n",
       "         0.00175334, -0.02320477],\n",
       "       [-0.02566138, -0.05334219, -0.03936853, ..., -0.00694489,\n",
       "        -0.12886402, -0.0103204 ],\n",
       "       ...,\n",
       "       [-0.01687125, -0.02210648,  0.00226213, ...,  0.03673438,\n",
       "        -0.03365464, -0.05304608],\n",
       "       [-0.07158102, -0.07273172,  0.01097328, ...,  0.04238746,\n",
       "        -0.05822933,  0.00168229],\n",
       "       [-0.06057917, -0.03957532,  0.04482635, ...,  0.00086623,\n",
       "        -0.07034633,  0.0659965 ]], shape=(21, 384), dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da1697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding without filtering the non-english language product \n",
    "\n",
    "emb_catalog = np.load(\"data/emb_catalog.npz\")[\"arr_0\"]\n",
    "emb_taxonomy = np.load(\"data/emb_taxonomy.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding with filtering the non-english language product \n",
    "\n",
    "\n",
    "# emb_catalog = np.load(\"data/embeddings_catalog.npy\")\n",
    "# emb_taxonomy = np.load(\"data/embeddings_taxonomy.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a69f10dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 214 brand that sell more than 100 product\n"
     ]
    }
   ],
   "source": [
    "df_nlp_copy = df_nlp.copy()\n",
    "df_nlp_copy = df_nlp_copy[df_nlp_copy[\"brand_encoded\"]!=0]\n",
    "\n",
    "value_count_brand = df_nlp_copy[\"brand_encoded\"].value_counts()\n",
    "\n",
    "value_count_brand = value_count_brand[value_count_brand > 100]\n",
    "\n",
    "print(f\"There is {len(value_count_brand)} brand that sell more than 100 product\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce79370",
   "metadata": {},
   "source": [
    "there is brand that sell multiple products, so we will keep brand information for each product entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d7893",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a94a8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 1. Scale price + brand\n",
    "# --------------------------\n",
    "price_scaler = MinMaxScaler()\n",
    "price_scaled = price_scaler.fit_transform(df_nlp[['sale_price']])\n",
    "\n",
    "brand_scaler = StandardScaler()\n",
    "brand_scaled = brand_scaler.fit_transform(df_nlp[['brand_encoded']])\n",
    "\n",
    "# --------------------------\n",
    "# 2. Combine reduced embedding + numeric features\n",
    "# --------------------------\n",
    "X_products = np.hstack([emb_catalog, price_scaled, brand_scaled])\n",
    "\n",
    "zeros_tax = np.zeros((emb_taxonomy.shape[0], 2))\n",
    "X_tax = np.hstack([emb_taxonomy, zeros_tax])\n",
    "\n",
    "# --------------------------\n",
    "# 3. Compute similarity matrix between catalogue and taxonomy embedings\n",
    "# --------------------------\n",
    "\n",
    "similarity_matrix = cosine_similarity(X_products, X_tax)\n",
    "\n",
    "# --------------------------\n",
    "# 4. Linear assignation algorithm\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2058c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_1_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apparel &amp; accessories</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>home &amp; garden</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furniture</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health &amp; beauty</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toys &amp; games</td>\n",
       "      <td>9879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>electronics</td>\n",
       "      <td>8094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sporting goods</td>\n",
       "      <td>4674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arts &amp; entertainment</td>\n",
       "      <td>4318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>luggage &amp; bags</td>\n",
       "      <td>4069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>media</td>\n",
       "      <td>3708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cameras &amp; optics</td>\n",
       "      <td>3359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>baby &amp; toddler</td>\n",
       "      <td>2425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>animals &amp; pet supplies</td>\n",
       "      <td>2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>software</td>\n",
       "      <td>1534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>food, beverages &amp; tobacco</td>\n",
       "      <td>1301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hardware</td>\n",
       "      <td>1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>office supplies</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vehicles &amp; parts</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>business &amp; industrial</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>religious &amp; ceremonial</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mature</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_1_name  count\n",
       "0       apparel & accessories  20000\n",
       "1               home & garden  20000\n",
       "2                   furniture  20000\n",
       "3             health & beauty  20000\n",
       "4                toys & games   9879\n",
       "5                 electronics   8094\n",
       "6              sporting goods   4674\n",
       "7        arts & entertainment   4318\n",
       "8              luggage & bags   4069\n",
       "9                       media   3708\n",
       "10           cameras & optics   3359\n",
       "11             baby & toddler   2425\n",
       "12     animals & pet supplies   2222\n",
       "13                   software   1534\n",
       "14  food, beverages & tobacco   1301\n",
       "15                   hardware   1122\n",
       "16            office supplies    737\n",
       "17           vehicles & parts    666\n",
       "18      business & industrial    132\n",
       "19     religious & ceremonial      8\n",
       "20                     mature      5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categories_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8ff88da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction du graphe pour 128253 produits et 21 catégories...\n",
      "Calcul des Top-K candidats...\n",
      "Création des arcs de coûts...\n",
      "Lancement du solveur OR-Tools...\n",
      "✅ Solution Optimale trouvée ! Coût total : 10359029364\n",
      "Exemple des 5 premières assignations : ['toys & games', 'apparel & accessories', 'toys & games', 'toys & games', 'toys & games']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & VÉRIFICATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Adapter selon les noms réels de tes colonnes dans df_categories_count\n",
    "COL_NAME_COUNT = 'count'         # Colonne contenant le nombre (quota)\n",
    "COL_NAME_CATEGORY = 'level_1_name' # Colonne contenant le nom de la catégorie\n",
    "\n",
    "# Vérification des dimensions\n",
    "n_products, n_cols_sim = similarity_matrix.shape\n",
    "n_cats_df = len(df_categories_count)\n",
    "\n",
    "if n_cols_sim != n_cats_df:\n",
    "    raise ValueError(f\"Erreur : La matrice a {n_cols_sim} colonnes mais le DF a {n_cats_df} catégories.\")\n",
    "\n",
    "# Extraction des quotas sous forme de tableau numpy\n",
    "quotas = df_categories_count[COL_NAME_COUNT].values.astype(int)\n",
    "\n",
    "# Vérification de la somme (doit être égale au nombre de produits)\n",
    "diff_quota = n_products - quotas.sum()\n",
    "if diff_quota != 0:\n",
    "    print(f\"⚠️ ATTENTION : La somme des quotas ({quotas.sum()}) diffère du nombre de produits ({n_products}).\")\n",
    "    print(f\"Correction automatique : Ajout/Retrait de {diff_quota} au quota de la dernière catégorie pour équilibrer.\")\n",
    "    quotas[-1] += diff_quota\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONSTRUCTION DU GRAPHE (Min-Cost Max-Flow)\n",
    "# ==============================================================================\n",
    "\n",
    "smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "\n",
    "# Indices des nœuds\n",
    "SOURCE = 0\n",
    "SINK = n_products + n_cats_df + 1\n",
    "START_PROD_NODES = 1\n",
    "START_CAT_NODES = n_products + 1\n",
    "\n",
    "# Paramètres d'optimisation\n",
    "# Ne pas demander plus de candidats TOP_K que le nombre réel de catégories.\n",
    "TOP_K = min(10, n_cats_df)  # On garde au maximum 100, ou moins si moins de catégories\n",
    "COST_MULTIPLIER = 100000     # Pour convertir les distances float en int\n",
    "\n",
    "print(f\"Construction du graphe pour {n_products} produits et {n_cats_df} catégories...\")\n",
    "\n",
    "# --- A. Arcs Source -> Produits ---\n",
    "# Tous les produits viennent de la source (Capacité 1, Coût 0)\n",
    "# Pour optimiser la boucle Python, on utilise des listes\n",
    "sources = [SOURCE] * n_products\n",
    "targets = list(range(START_PROD_NODES, START_PROD_NODES + n_products))\n",
    "capacities = [1] * n_products\n",
    "costs = [0] * n_products\n",
    "smcf.add_arcs_with_capacity_and_unit_cost(sources, targets, capacities, costs)\n",
    "\n",
    "# --- B. Arcs Produits -> Catégories (Optimisation Top-K) ---\n",
    "# On identifie les K meilleures catégories pour chaque produit pour éviter de créer 4 millions d'arcs\n",
    "print(\"Calcul des Top-K candidats...\")\n",
    "# argpartition met les K plus grands indices à la fin de chaque ligne\n",
    "top_k_indices = np.argpartition(similarity_matrix, -TOP_K, axis=1)[:, -TOP_K:]\n",
    "\n",
    "print(\"Création des arcs de coûts...\")\n",
    "# On prépare les listes pour le chargement en masse (batch)\n",
    "u_list, v_list, cap_list, cost_list = [], [], [], []\n",
    "\n",
    "for i in range(n_products):\n",
    "    candidates = top_k_indices[i]\n",
    "    row_scores = similarity_matrix[i] # Accès plus rapide\n",
    "    \n",
    "    for cat_idx in candidates:\n",
    "        score = row_scores[cat_idx]\n",
    "        \n",
    "        # Coût = (1 - Similarité) converti en entier\n",
    "        # On veut minimiser le coût, donc maximiser la similarité\n",
    "        int_cost = int((1.0 - score) * COST_MULTIPLIER)\n",
    "        \n",
    "        u_list.append(START_PROD_NODES + i)\n",
    "        v_list.append(START_CAT_NODES + int(cat_idx))\n",
    "        cap_list.append(1)\n",
    "        cost_list.append(int_cost)\n",
    "\n",
    "smcf.add_arcs_with_capacity_and_unit_cost(u_list, v_list, cap_list, cost_list)\n",
    "\n",
    "# --- C. Arcs Catégories -> Puits ---\n",
    "# Chaque catégorie a une capacité égale à son QUOTA\n",
    "u_sink, v_sink, cap_sink, cost_sink = [], [], [], []\n",
    "\n",
    "for j, quota in enumerate(quotas):\n",
    "    u_sink.append(START_CAT_NODES + j)\n",
    "    v_sink.append(SINK)\n",
    "    cap_sink.append(int(quota))\n",
    "    cost_sink.append(0)\n",
    "\n",
    "smcf.add_arcs_with_capacity_and_unit_cost(u_sink, v_sink, cap_sink, cost_sink)\n",
    "\n",
    "# --- D. Définition du Flux ---\n",
    "smcf.set_node_supply(SOURCE, int(n_products))\n",
    "smcf.set_node_supply(SINK, -int(n_products))\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. RÉSOLUTION\n",
    "# ==============================================================================\n",
    "print(\"Lancement du solveur OR-Tools...\")\n",
    "status = smcf.solve()\n",
    "\n",
    "if status == smcf.OPTIMAL:\n",
    "    print(f\"✅ Solution Optimale trouvée ! Coût total : {smcf.optimal_cost()}\")\n",
    "    \n",
    "    # Récupération des résultats\n",
    "    final_assignments = np.zeros(n_products, dtype=int)\n",
    "    \n",
    "    # On parcourt les arcs pour voir où le flux est passé\n",
    "    # Note : C'est plus rapide de parcourir les arcs internes que tous les arcs\n",
    "    for i in range(smcf.num_arcs()):\n",
    "        if smcf.flow(i) > 0:\n",
    "            u = smcf.tail(i)\n",
    "            v = smcf.head(i)\n",
    "            # On cherche uniquement les arcs Produit -> Catégorie\n",
    "            if u >= START_PROD_NODES and u < START_CAT_NODES:\n",
    "                p_idx = u - START_PROD_NODES\n",
    "                c_idx = v - START_CAT_NODES\n",
    "                final_assignments[p_idx] = c_idx\n",
    "\n",
    "    # Traduction Index -> Nom de catégorie\n",
    "    # On assume que l'index 0 de quotas correspond à l'index 0 de df_categories_count\n",
    "    cat_names_map = df_categories_count[COL_NAME_CATEGORY].values\n",
    "    predicted_labels = [cat_names_map[i] for i in final_assignments]\n",
    "    \n",
    "    # Ajout au DataFrame original (si tu l'as sous la main)\n",
    "    # df_nlp['final_category'] = predicted_labels\n",
    "    \n",
    "    print(\"Exemple des 5 premières assignations :\", predicted_labels[:5])\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Pas de solution trouvée. Vérifiez que la somme des quotas est exacte et que Top-K est suffisant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6faa06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(data= predicted_labels , columns=[\"predicted_level_1_cat\"])\n",
    "df_result[\"hashed_external_id\"] = df_nlp[\"hashed_external_id\"]\n",
    "df_result.to_csv(\"data/predicted_level_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9d983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
